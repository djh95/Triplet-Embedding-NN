{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1a6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from jupyterplot import ProgressPlot\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from Define import *\n",
    "import NUS_WIDE as nus\n",
    "import TenNet as TenNet\n",
    "import PredictionModel as P\n",
    "from TripletLossFunc import TripletLossFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfad4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "getTenNetFromFile = False\n",
    "getDecoderFromFile = False\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2754758",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    Number_Of_Images_Train = int(400/BATCH_SIZE) * BATCH_SIZE\n",
    "\n",
    "#NUS_WIDE\n",
    "train_data = nus.NUS_WIDE_Helper(nus.DataSetType.Train_81, Number_Of_Images_Train)\n",
    "valid_data = nus.NUS_WIDE_Helper(nus.DataSetType.Test_81,  Number_Of_Images_Valid)\n",
    "#test_data = NUS_WIDE_Helper(DataSetType.Test_81)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ab8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    image_model = models.vgg11(pretrained=True).to(device)\n",
    "else:\n",
    "    image_model = models.vgg16(pretrained=True).to(device)\n",
    "    \n",
    "#image_model = TenNet_Image().to(device)\n",
    "tag_model = TenNet.TenNet_Tag(train_data.get_tag_list()).to(device)\n",
    "\n",
    "optim = torch.optim.Adam([{'params' : image_model.parameters()}, {'params' : tag_model.parameters()}], lr=0.0001)\n",
    "triplet_loss = TripletLossFunc(Margin_Distance)\n",
    "\n",
    "n_epochs = 20\n",
    "Lambda = 0.01\n",
    "min_valid_loss = -1\n",
    "ten_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e267b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b411ca615d094048828fd01372c72d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0: 1-train dataset with train model\n",
      "loss: 157.14,  IT_pos_dis: 38.27,  II_pos_dis: 23.87,  IT_neg_dis: 38.60,  II_neg_dis: 23.43\n",
      " \n",
      "epoch:0: 2-valid dataset with evalue model\n",
      "loss: 65.12,  IT_pos_dis: 10.55,  II_pos_dis: 1.14,  IT_neg_dis: 10.55,  II_neg_dis: 1.14\n",
      " \n",
      "epoch:1: 1-train dataset with train model\n",
      "loss: 94.64,  IT_pos_dis: 22.63,  II_pos_dis: 9.60,  IT_neg_dis: 22.98,  II_neg_dis: 9.72\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11024/2170655653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mTenNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_loss_dis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch:{e}: 1-train dataset with train model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_dis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss_dis_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTenNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_valid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mTenNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_loss_dis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch:{e}: 2-valid dataset with evalue model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_dis_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmin_valid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_dis_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\code\\python\\TENN\\Triplet-Embedding-NN\\code\\TenNet\\TenNetUtils.py\u001b[0m in \u001b[0;36mevalue\u001b[1;34m(image_model, tag_model, loader, triplet_loss, Lambda, optim, epoch, min_loss, save_best)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_epoch_computation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_best\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmin_loss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\code\\python\\TENN\\Triplet-Embedding-NN\\code\\TenNet\\TenNetUtils.py\u001b[0m in \u001b[0;36msingle_epoch_computation\u001b[1;34m(image_model, tag_model, loader, triplet_loss, Lambda, optim, updata)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mupdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\code\\python\\TENN\\Triplet-Embedding-NN\\code\\TenNet\\TenNetUtils.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(x_images, y_tags, image_model, tag_model, triplet_loss, Lambda)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mimage_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtag_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if getTenNetFromFile:\n",
    "    TenNet.getTenModel(tag_model, image_model, name = \"SavedModelState/IT_model.ckpt\")\n",
    "else:\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for e in pbar:\n",
    "    \n",
    "        print(f\"epoch:{e}:\")\n",
    "        print(\"1-7 use the same weights, since just 1 updates the weights.\\n\")\n",
    "        loss_dis_valid = TenNet.evalue(image_model, tag_model, valid_loader, triplet_loss, Lambda, optim, e, min_valid_loss, True)   \n",
    "        TenNet.output_loss_dis(f\" 2-valid dataset with evalue model\", loss_dis_valid)\n",
    "        min_valid_loss = loss_dis_valid[5]\n",
    "        '''\n",
    "        loss_dis_train = TenNet.evalue(image_model, tag_model, train_loader, triplet_loss, Lambda, optim, e, min_valid_loss, True)   \n",
    "        TenNet.output_loss_dis(f\" 3-train dataset with evalue model\", loss_dis_train)\n",
    "        \n",
    "        print(\"2-3 use the same way to evalue the model (both eval() and no_grad()), but different data sets.\\n\")\n",
    "        \n",
    "        loss_dis_train = TenNet.evalue2(image_model, tag_model, train_loader, triplet_loss, Lambda, optim)   \n",
    "        TenNet.output_loss_dis(f\" 4-train dataset with evalue model, just evalue model\", loss_dis_train)\n",
    "        \n",
    "        loss_dis_train = TenNet.evalue3(image_model, tag_model, train_loader, triplet_loss, Lambda, optim)   \n",
    "        TenNet.output_loss_dis(f\" 5-train dataset with evalue model, just no gradient\", loss_dis_train)\n",
    "        \n",
    "        print(\"4-5 use the same data set, but different ways to train the model.One uses eval(), one uses no_grad().\\n\")\n",
    "        \n",
    "        loss_dis_train =  TenNet.train(image_model, tag_model, train_loader, triplet_loss, Lambda, optim, False)\n",
    "        TenNet.output_loss_dis(f\" 6-train dataset with train model, but doesn't updata\", loss_dis_train)\n",
    "        \n",
    "        loss_dis_train =  TenNet.train(image_model, tag_model, train_loader, triplet_loss, Lambda, optim, False)\n",
    "        TenNet.output_loss_dis(f\" 6-train dataset with train model, but doesn't updata\", loss_dis_train)\n",
    "        '''\n",
    "        \n",
    "        loss_dis_train =  TenNet.train(image_model, tag_model, train_loader, triplet_loss, Lambda, optim)\n",
    "        TenNet.output_loss_dis(f\" 1-train dataset with train model\", loss_dis_train)\n",
    "        \n",
    "        print(\"6-6-1 use the same data set and way to train the model, but 6 doesn't updata the weight.\\n\")\n",
    "        \n",
    "        ten_res.append([loss_dis_train,loss_dis_valid])\n",
    "        \n",
    "        if test and e == 2:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ten_res\n",
    "TenNet.printLossLog(res, n_epochs)\n",
    "TenNet.printLossProgressPlot(res, n_epochs)\n",
    "TenNet.printDistanceProgressPlot(res, n_epochs, train=True)\n",
    "TenNet.printDistanceProgressPlot(res, n_epochs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2ba40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ceea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = P.TagDecoder(train_data.get_tag_num()).to(device)\n",
    "\n",
    "optim = torch.optim.Adam(predict_model.parameters(), lr=0.001)\n",
    "loss_funk = F.pairwise_distance \n",
    "\n",
    "n_epochs = N_Epochs_Decoder\n",
    "max_accuracy = -1\n",
    "threshold = 0.5\n",
    "decoder_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if getDecoderFromFile:\n",
    "    P.getDecoderModel(predict_model, name = \"SavedModelState/decoder_model.ckpt\")\n",
    "else:\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for e in pbar:\n",
    "    \n",
    "        loss_num_train = P.train(predict_model, tag_model, image_model, train_loader, loss_funk, optim, threshold)\n",
    "        P.output_loss_num(f\"epoch:{e}: 1-train dataset with train model\", loss_num_train)\n",
    "        \n",
    "        loss_num_valid = P.predict(predict_model, tag_model, image_model, valid_loader, loss_funk, optim, threshold, True, max_accuracy, e)   \n",
    "        P.output_loss_num(f\"epoch:{e}: 2-valid dataset with evalue model\", loss_num_valid)\n",
    "        max_accuracy = loss_num_valid[4]\n",
    "    \n",
    "        decoder_res.append([loss_num_train,loss_num_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bda6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    pp = ProgressPlot(plot_names=[\"loss\", \"mean num of tags\", \"accuracy\"], \n",
    "                      line_names=[\"train/correct\", \"valid/total\"],\n",
    "                      x_lim=[0, n_epochs-1], \n",
    "                      y_lim=[[0,50], [0,20], [0,1]])\n",
    "P.printResult(decoder_res, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd7cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650a6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 21, 21, 17, 18, 19, 25, 29, 20, 20, 9, 27, 29, 22, 17, 20, 15, 26, 26, 18, 19, 20, 26, 22, 23, 26, 25, 29, 27, 28, 29, 30] [8, 2, 16, 21, 29, 22, 12, 29, 7, 30, 17, 25, 11, 27, 13, 29, 15, 29, 17, 24, 19, 29, 29, 29, 23, 24, 27, 29, 27, 28, 29, 30]\n",
      "tensor([ 4.6643e-01,  1.3243e-01,  1.9441e-01, -4.7355e-01, -1.4267e-01,\n",
      "        -3.4867e-01,  2.6157e-02, -2.7753e-01,  5.1259e-02,  2.3448e-01,\n",
      "         8.6683e-02,  6.0205e-02, -7.1069e-01, -2.8344e-01, -3.3232e-01,\n",
      "         1.5805e-01,  2.1986e-01,  1.7727e-01, -1.7434e-01,  6.6996e-01,\n",
      "        -4.2656e-01,  1.1245e-01, -3.2373e-01, -4.6245e-01, -3.1490e-01,\n",
      "        -3.7352e-02, -2.8791e-01, -1.7289e-01,  6.5100e-01, -2.4598e-01,\n",
      "        -2.4802e-01, -7.0213e-01,  2.2615e-01, -5.0316e-01, -7.3564e-01,\n",
      "        -3.9543e-01, -2.4660e-01, -1.8978e-01,  3.4221e-01, -3.5239e-01,\n",
      "         1.0371e-01,  2.8671e-01, -2.2580e-02, -1.6176e+00,  1.3838e-01,\n",
      "        -5.9134e-01,  1.1898e-01,  1.3564e-02, -3.3850e-01, -3.5049e-01,\n",
      "         2.5211e-01, -4.7056e-01, -2.4324e-01, -7.5950e-01,  8.5317e-03,\n",
      "        -5.0451e-03,  9.6699e-02,  1.7205e-02, -2.2696e-01,  3.3939e-01,\n",
      "         4.4195e-01,  8.3131e-01, -6.2655e-01, -3.6632e-01, -2.2256e-01,\n",
      "        -2.0223e-01,  8.5618e-01,  5.6926e-01,  5.8609e-01, -4.5236e-01,\n",
      "         1.3078e-01, -4.4290e-01, -8.7021e-01, -2.8461e-01,  1.8143e-01,\n",
      "         1.3812e-01, -2.8760e-01, -3.9998e-01,  1.4843e-01,  1.4348e-01,\n",
      "         5.7963e-01, -1.8413e-01, -8.6223e-02, -6.4933e-01,  3.3548e-01,\n",
      "         7.0101e-01, -1.5594e-01,  1.7204e-01,  5.9824e-01, -1.9232e-01,\n",
      "        -2.1921e-01, -1.6258e-01, -2.3070e-01, -9.4194e-01, -2.7535e-01,\n",
      "        -2.7033e-01, -5.6682e-01, -5.3283e-01, -5.1350e-01,  2.9278e-01,\n",
      "        -1.0689e-01,  3.5015e-02,  9.0804e-02,  1.4634e-01, -8.7145e-02,\n",
      "         3.8442e-01, -2.4952e-01,  5.6828e-01,  6.9562e-01,  1.6817e-01,\n",
      "        -2.5778e-01,  2.1431e-01, -3.4284e-01, -1.8484e-01, -5.2642e-02,\n",
      "         7.9371e-01, -1.0869e-01, -2.8872e-01,  9.0725e-01,  1.0355e-01,\n",
      "         1.9856e-02, -2.3898e-01, -2.8889e-01, -5.3624e-01, -1.7047e-01,\n",
      "        -7.6081e-02, -3.1462e-01,  7.0121e-02, -4.5132e-01, -4.7630e-01,\n",
      "        -1.6617e-01, -4.9558e-01,  3.8484e-01, -7.0832e-01,  1.7542e-01,\n",
      "         6.7455e-02,  6.2039e-01, -4.3135e-01, -2.7255e-01, -4.2004e-01,\n",
      "        -7.2981e-01,  7.4350e-01,  5.6235e-01,  6.5928e-02, -2.6209e-01,\n",
      "         3.7899e-01,  3.2750e-01,  1.2283e-03, -8.7364e-01,  2.2925e-01,\n",
      "        -2.6740e-01,  1.1978e-01, -4.9739e-01,  4.1002e-02, -2.1681e-01,\n",
      "        -5.0322e-02,  3.1311e-02,  8.2636e-01, -5.7534e-01,  2.5261e-01,\n",
      "        -3.6008e-02,  4.3293e-01,  1.1761e+00, -6.6572e-01, -6.8691e-01,\n",
      "        -4.4124e-01,  2.5578e-01, -1.8752e-01,  9.7996e-02, -2.1092e-01,\n",
      "         3.9166e-01,  2.1707e-02, -7.1885e-01,  8.7577e-01, -8.2680e-02,\n",
      "        -1.6085e-02, -3.8820e-01, -9.4076e-02,  1.4304e-01,  3.5291e-01,\n",
      "        -9.1262e-02, -2.2670e-02, -2.5638e-01, -5.6796e-03, -4.1104e-01,\n",
      "         7.3954e-01, -5.1353e-01,  2.2180e-01,  1.3800e-01,  1.7723e-01,\n",
      "         1.5452e-01, -3.3123e-01,  1.1609e-01, -6.9642e-02,  5.2457e-02,\n",
      "        -3.3863e-01, -3.6115e-01, -4.1745e-01, -5.9695e-03,  5.1092e-01,\n",
      "        -2.6885e-02,  1.4763e-01, -5.9271e-01, -2.5118e-01,  2.8464e-01,\n",
      "         6.0655e-01, -6.9484e-01,  7.0262e-01, -1.6285e-01,  3.2418e-02,\n",
      "        -1.6331e-01, -1.0057e-01, -1.5478e-01,  6.7706e-01, -4.1235e-02,\n",
      "         3.5300e-01,  3.9543e-01, -1.6778e-01,  9.0837e-02,  1.6303e-01,\n",
      "        -3.7660e-02,  3.1917e-01, -1.1595e-01,  3.0258e-01, -1.6374e-01,\n",
      "        -3.5496e-01, -1.1723e-02,  5.0050e-01,  4.6124e-01, -2.0396e-01,\n",
      "        -9.5927e-03, -6.2874e-01,  4.6377e-01,  3.6709e-01, -5.3310e-01,\n",
      "         7.6078e-01, -4.5801e-01, -1.5125e-01, -5.2825e-01, -6.8828e-01,\n",
      "        -2.1490e-01, -2.2644e-02, -1.7045e-01, -7.3083e-01,  1.5745e-03,\n",
      "        -2.4127e-01, -2.6549e-01, -1.2007e-01,  1.2501e-01,  3.3368e-01,\n",
      "        -2.7103e-01, -1.2041e-02,  4.8451e-01,  1.4602e-01,  3.4502e-01,\n",
      "         6.6441e-02, -2.0582e-01, -4.3689e-01, -3.6259e-01, -4.8303e-01,\n",
      "        -1.3622e-01, -1.4423e-01,  2.2540e-01,  8.8666e-01, -2.1479e-01,\n",
      "        -1.9738e-01,  6.2673e-01,  6.4879e-01, -4.1632e-01,  5.6971e-01,\n",
      "        -2.6237e-01, -3.8667e-01, -2.4890e-01,  1.2286e-01,  2.9392e-01,\n",
      "        -2.1735e-01,  3.2188e-02, -1.9619e-01, -2.2879e-01, -6.4202e-02,\n",
      "         5.9975e-02, -2.4369e-01, -2.8536e-01, -2.0662e-01,  5.1701e-01,\n",
      "         1.6867e-01,  2.7415e-01,  5.0791e-01, -8.9248e-02, -3.8048e-02,\n",
      "         5.1187e-01,  2.4245e-01,  2.2452e-01,  5.8418e-02, -8.6023e-01,\n",
      "         1.7865e-01, -8.4755e-02, -2.9513e-01,  6.5048e-02, -1.3060e-01,\n",
      "        -5.7594e-01,  2.9386e-01,  3.3194e-01, -8.1610e-02, -5.3555e-02,\n",
      "        -7.3968e-01, -2.1644e-01,  7.5616e-01, -2.6560e-01,  2.2629e-01,\n",
      "         5.0149e-01, -1.0439e-02,  1.2345e-01, -3.4458e-01, -3.1150e-01,\n",
      "        -1.4819e-01,  1.0132e+00, -4.1308e-01, -9.2190e-03,  1.0365e-01,\n",
      "         1.7807e-01,  5.0670e-01, -5.0956e-01,  1.0859e-01, -4.0592e-01,\n",
      "         6.5248e-02, -5.6437e-01,  6.9605e-01,  9.4811e-01,  3.1835e-01,\n",
      "         4.3705e-01,  2.2500e-01,  4.2762e-01,  5.4969e-01, -1.0541e-01,\n",
      "        -2.9469e-01,  4.4234e-01, -2.7056e-01,  9.7321e-03, -4.6208e-01,\n",
      "         1.7064e-01, -1.1357e-01, -6.9759e-01,  2.7908e-01,  2.6836e-02,\n",
      "        -5.5360e-02, -1.1375e-01,  1.8996e-01, -2.4343e-01, -7.3656e-01,\n",
      "         2.0515e-01,  4.9438e-01, -9.2061e-02,  1.5124e-01,  1.5011e-01,\n",
      "         1.7274e-01,  1.6399e-01,  1.7169e-01,  4.5441e-01, -2.1051e-02,\n",
      "        -4.0577e-01,  6.2688e-01,  3.1729e-01,  6.1551e-01, -3.4775e-01,\n",
      "        -2.4808e-01,  3.3772e-01, -3.1839e-01, -2.0056e-01, -5.8271e-01,\n",
      "         2.0634e-01,  2.3890e-01,  1.6226e-01, -3.0676e-01, -2.9920e-01,\n",
      "        -1.6845e-02, -1.3886e-01, -1.6997e-01,  2.7565e-01, -1.1138e-01,\n",
      "        -6.9088e-01, -6.0640e-02,  4.9332e-01,  2.4416e-01, -4.3984e-02,\n",
      "        -3.1588e-02, -5.6861e-01,  1.7440e-01,  7.0451e-01, -6.6021e-01,\n",
      "         3.0259e-01,  6.6527e-01,  7.7535e-01, -3.1962e-01,  3.2132e-01,\n",
      "        -1.5460e-01,  4.8677e-01, -1.9403e-01, -1.3034e-01,  2.9344e-01,\n",
      "        -3.1398e-01,  6.5729e-01, -5.3281e-01, -1.0480e-01,  8.3799e-02,\n",
      "         3.8220e-01, -3.3060e-01, -1.4559e-01, -9.1044e-03,  5.3451e-01,\n",
      "         3.4536e-01, -1.5906e-01, -2.0776e-02,  4.9532e-03,  4.1117e-02,\n",
      "         2.0401e-01, -3.5268e-02, -4.1330e-02, -1.5169e-01, -2.4876e-01,\n",
      "        -3.4552e-01, -1.0086e+00,  2.4226e-02,  4.0707e-01, -4.0085e-02,\n",
      "        -3.0087e-01, -1.1669e-01, -3.6904e-01,  3.8381e-01,  7.1795e-01,\n",
      "        -1.5891e-01,  8.4184e-01, -7.8400e-01,  4.2542e-01,  5.9305e-02,\n",
      "        -6.4511e-01,  1.1147e-01, -6.7811e-01,  5.5225e-02, -6.5633e-01,\n",
      "         3.9133e-01, -3.7446e-02, -5.0679e-01, -5.7507e-02, -4.4808e-02,\n",
      "         3.6555e-01,  1.8308e-01, -3.6783e-01, -5.6028e-01, -1.3133e-01,\n",
      "        -1.6899e-01, -2.1292e-01, -2.8288e-02,  2.8978e-01, -3.1502e-01,\n",
      "         1.9070e-01, -6.7904e-01, -4.5933e-01,  3.7748e-01,  9.1694e-01,\n",
      "        -4.7315e-02,  5.7463e-02, -4.1308e-01, -1.8133e-01,  1.4623e+00,\n",
      "        -8.3257e-02, -9.1722e-02, -3.5444e-01, -2.1583e-01,  1.3006e-01,\n",
      "        -1.7304e-01, -7.6150e-02, -4.2945e-01,  4.3994e-01,  7.6978e-02,\n",
      "        -5.0721e-01, -9.5945e-01, -1.0768e-01,  2.2005e-02,  2.7501e-01,\n",
      "        -3.9644e-01,  1.0522e-01,  5.8406e-02, -1.4196e-01,  2.3745e-01,\n",
      "         4.1463e-02, -3.2108e-01, -7.6504e-02, -4.9358e-01, -5.0646e-01,\n",
      "        -5.6811e-02, -1.8662e-01, -6.5036e-02,  8.8198e-01,  2.1939e-01,\n",
      "        -5.5300e-01,  4.1328e-02,  4.3867e-01,  5.7450e-01,  2.8680e-01,\n",
      "        -2.3906e-01,  1.4672e-01,  4.5896e-01,  6.9445e-01,  1.2098e-01,\n",
      "        -2.0218e-01, -5.6309e-01, -1.8666e-02,  1.0601e+00, -5.4388e-01,\n",
      "        -7.8959e-02,  1.8411e-01,  3.3543e-01,  1.8364e-01, -8.6120e-01,\n",
      "        -5.7210e-02, -3.5999e-01,  3.7819e-01, -5.8552e-01,  4.5188e-01,\n",
      "         6.8649e-02, -2.3084e-01, -5.9271e-01, -2.3667e-01,  2.0272e-01,\n",
      "        -4.9623e-02, -1.9958e-01, -2.8318e-01, -3.3854e-01,  3.7879e-01,\n",
      "         2.7191e-01, -4.9501e-01,  1.3786e-01, -1.6469e-01,  4.3253e-02,\n",
      "         1.4108e-01,  5.6797e-01, -2.4452e-01, -1.1577e-02,  4.8390e-02,\n",
      "         3.4581e-01,  3.7266e-01, -2.6438e-01,  6.5905e-01,  1.0045e-01,\n",
      "        -3.3113e-01, -5.1376e-01, -8.7408e-02, -4.0985e-01, -3.9166e-01,\n",
      "         5.0943e-01, -1.8470e-01,  7.4662e-01,  8.5901e-02,  6.5725e-02,\n",
      "         7.9457e-01, -1.6202e-01, -6.7042e-01, -1.5510e-01,  1.8924e-01,\n",
      "        -4.1940e-01, -7.5622e-02,  2.9013e-01, -8.2142e-01, -2.8550e-01,\n",
      "         2.1448e-01, -9.7325e-01,  3.6596e-01,  1.2144e-01,  2.0681e-01,\n",
      "         5.2128e-01,  3.6340e-01,  4.5056e-02,  5.0906e-02, -1.5337e-01,\n",
      "        -9.0426e-01,  1.9809e-01,  1.4815e-02, -5.0297e-01,  1.2214e-01,\n",
      "        -7.1179e-01, -8.4638e-01, -3.6164e-01, -9.1406e-02, -3.3526e-01,\n",
      "         4.9473e-01,  2.0870e-01,  3.5000e-01, -9.7066e-02, -3.0898e-01,\n",
      "        -3.5059e-01,  2.4733e-01,  1.6269e-01,  5.4918e-01, -2.8414e-01,\n",
      "         1.3027e-02, -2.2776e-01, -1.7027e-02, -1.0700e-01, -1.9654e-01,\n",
      "         4.6207e-01,  2.9365e-01,  1.0732e-01, -3.0166e-01,  3.2703e-01,\n",
      "        -3.9151e-03,  4.4888e-01, -4.6754e-01,  5.0118e-01,  4.3484e-01,\n",
      "        -2.1264e-01,  1.7330e-01, -1.1658e-01,  8.0698e-02, -1.9184e-01,\n",
      "        -5.2211e-01,  3.5061e-01, -5.1172e-01,  9.7973e-02, -2.6879e-01,\n",
      "        -1.5190e-01, -2.9309e-01,  5.4741e-02,  8.3137e-01,  3.4883e-01,\n",
      "        -2.9567e-01,  1.5302e-01, -4.4086e-01,  3.0051e-01, -3.8274e-01,\n",
      "        -1.5806e-01, -7.5295e-02,  2.0403e-01, -6.2760e-02, -3.6952e-01,\n",
      "         1.5700e-01,  4.4425e-02, -6.0033e-01, -4.9056e-01, -1.2891e-01,\n",
      "        -1.4529e-01, -7.1588e-01, -5.1562e-01, -6.6456e-01,  2.9308e-01,\n",
      "         2.0486e-01,  8.5498e-02,  3.4730e-01,  5.3993e-01, -8.8666e-01,\n",
      "         3.3207e-02,  2.8747e-01, -7.0505e-01,  8.9020e-02, -8.7423e-02,\n",
      "        -7.5401e-02, -5.4052e-01, -5.1966e-01,  2.6086e-01, -4.9803e-01,\n",
      "         5.0373e-02, -1.9078e-01, -8.0610e-03, -5.0534e-01, -1.0893e-01,\n",
      "        -5.7835e-01,  3.4965e-01, -1.3409e-01,  6.6771e-02, -1.2680e-01,\n",
      "         2.8298e-01, -1.2515e-01, -8.1238e-01,  6.5873e-01, -5.9604e-01,\n",
      "        -1.0958e-01, -8.5359e-01,  1.8111e-01, -5.7363e-02,  3.9883e-01,\n",
      "         2.6401e-01, -1.9001e-01,  6.5977e-01, -4.6307e-02,  6.0074e-01,\n",
      "        -3.6343e-01, -1.0718e-01, -9.5081e-02, -1.5706e-01,  3.2433e-01,\n",
      "         6.2396e-03, -5.8817e-02,  2.7273e-01,  4.4890e-01,  4.7276e-02,\n",
      "        -3.7002e-01, -3.5230e-01, -1.4514e-01,  7.6785e-02, -1.7671e-01,\n",
      "        -4.6878e-01, -2.1814e-01,  1.4104e-02,  8.1429e-01, -2.1127e-02,\n",
      "        -2.6176e-02, -4.4218e-01, -3.0128e-01, -7.7385e-02, -2.3444e-01,\n",
      "         1.1728e-01,  2.9206e-01,  1.6898e-03,  2.3835e-01, -2.4196e-01,\n",
      "         4.2735e-01, -7.8189e-02, -5.5587e-01, -4.1534e-01, -4.3513e-01,\n",
      "        -9.4943e-03, -2.7129e-01, -1.5089e-01,  2.0742e-01,  3.5024e-01,\n",
      "         1.3978e-01, -1.6673e-01, -8.2775e-02, -3.1797e-01,  5.9303e-01,\n",
      "        -2.5473e-01,  3.8770e-01,  5.5014e-02, -4.1937e-01,  1.5967e-01,\n",
      "        -4.3292e-01, -3.0063e-01, -1.7049e-01, -2.5407e-01, -1.8005e-02,\n",
      "         2.3652e-01, -5.5627e-02,  1.1852e-01, -3.9549e-01,  2.4095e-01,\n",
      "        -1.4420e-02, -6.3790e-01,  9.8435e-02,  2.5995e-01, -7.2296e-01,\n",
      "         1.1455e+00, -5.9732e-01,  1.9169e-01, -2.4289e-01,  4.7676e-01,\n",
      "        -2.8728e-01,  2.5365e-01, -4.2677e-01,  3.5832e-01, -5.1148e-01,\n",
      "        -7.7107e-02, -4.4623e-01,  7.4635e-01,  4.7096e-01,  1.2262e-01,\n",
      "        -8.0193e-01, -1.0759e-01, -1.2512e-01,  1.8110e-01, -1.3107e-01,\n",
      "         1.9504e-01, -3.6710e-01,  3.3944e-02,  3.0371e-01,  1.3023e+00,\n",
      "        -6.2565e-02,  7.4629e-02,  1.2584e-01,  6.8575e-01,  2.7922e-01,\n",
      "         2.4001e-01, -2.3187e-02, -5.0741e-01, -5.0043e-02, -4.0420e-01,\n",
      "         1.8711e-01, -7.4319e-01, -3.0776e-01, -6.1452e-01,  2.7681e-01,\n",
      "         3.1509e-01,  6.3859e-01, -2.9316e-01, -1.2335e-02, -3.1080e-01,\n",
      "         4.4771e-01, -4.1503e-01,  5.7351e-01, -4.6831e-01, -2.4406e-01,\n",
      "         1.9752e-01,  4.8055e-01, -2.0885e-01,  7.4219e-01,  2.0063e-01,\n",
      "         1.6872e-01,  4.2309e-01,  2.0288e-01, -1.3183e+00,  1.4055e-01,\n",
      "        -6.1347e-01, -5.6034e-01,  4.9702e-01, -3.5040e-01, -5.6836e-01,\n",
      "         3.4537e-02, -4.6730e-01, -4.3853e-01, -5.8544e-01, -3.8109e-01,\n",
      "        -5.6144e-01, -4.8880e-04, -2.5875e-01, -3.9799e-01,  2.1109e-01,\n",
      "         7.4963e-02,  3.5921e-01, -1.5306e-01, -2.8842e-01,  2.8586e-01,\n",
      "         9.7452e-01, -8.6322e-02, -3.1241e-02,  8.7347e-01,  1.0341e-01,\n",
      "        -4.7077e-01, -1.2670e-01, -3.3127e-01,  2.7224e-01, -1.4683e-01,\n",
      "         7.0462e-01,  5.4890e-01,  3.9966e-01,  6.2694e-01, -1.3327e-01,\n",
      "         3.0983e-01, -6.9822e-03, -9.3893e-02, -2.9551e-01, -4.5035e-01,\n",
      "         3.7181e-01, -6.9477e-01,  4.0891e-01, -8.2134e-02, -2.1491e-01,\n",
      "        -3.6778e-01,  2.5864e-01, -5.8136e-01, -5.8015e-01,  6.8044e-01,\n",
      "        -2.3543e-03, -1.2355e-01,  5.4422e-01, -2.1031e-01,  5.8866e-01,\n",
      "         2.4246e-02,  1.9688e-01,  6.2492e-01, -1.8056e-01,  4.9031e-02,\n",
      "         4.2453e-01, -1.9254e-02,  1.1416e-01, -4.5839e-01,  6.3795e-01,\n",
      "        -2.0894e-01,  6.7402e-02, -1.4858e-01,  5.1007e-01,  8.1069e-01,\n",
      "        -1.5775e-01, -6.6265e-02,  2.5382e-01,  5.2696e-01, -7.1516e-01,\n",
      "        -4.4947e-01, -1.9868e-01,  3.4293e-02, -3.2607e-01, -1.4283e-01,\n",
      "         3.3740e-01,  1.7898e-04,  4.7186e-01,  2.8784e-01,  3.5907e-01,\n",
      "        -4.1708e-01,  2.4778e-01,  1.4610e-01, -3.8506e-01, -5.6852e-01,\n",
      "         9.6624e-02,  4.5263e-01, -3.7400e-01,  4.6534e-01, -5.2439e-01,\n",
      "         1.3358e-01, -1.1662e-01, -2.2505e-01, -9.7198e-02,  6.4990e-01,\n",
      "        -2.5526e-01,  4.0576e-02, -2.2900e-01, -4.6212e-02, -1.3587e-01,\n",
      "         2.7633e-01, -1.2457e-01,  3.6337e-01,  4.5829e-01, -1.3065e-01,\n",
      "         2.0366e-01, -9.9478e-02, -7.2711e-01, -6.0093e-01,  5.4719e-01,\n",
      "        -4.9901e-01,  4.2702e-01,  6.0800e-01,  5.8122e-01,  1.0494e-01,\n",
      "        -9.0235e-02,  3.7426e-01, -3.2144e-01,  2.8751e-01, -4.3610e-04,\n",
      "        -3.4077e-01, -2.1031e-01, -9.4062e-02,  3.0443e-01, -4.1461e-01,\n",
      "        -6.9725e-01, -2.9585e-01, -5.0375e-01,  3.5995e-01, -3.0440e-01,\n",
      "        -5.3190e-01, -1.4220e-01,  3.3154e-01, -5.1557e-01,  2.1146e-01,\n",
      "        -7.2812e-01, -1.8876e-01,  6.9421e-01, -1.1577e-01, -5.2829e-02,\n",
      "         1.9313e-01,  5.0654e-01,  3.0353e-02, -2.9323e-01, -2.3894e-01,\n",
      "        -6.3938e-01,  5.2184e-02, -2.4361e-01,  1.2755e-01, -1.2801e-02,\n",
      "        -4.6781e-01, -1.6988e-01,  2.5253e-01,  4.2390e-02, -2.0750e-01,\n",
      "        -1.6235e-01,  7.8649e-02, -3.2089e-01,  5.5629e-01, -1.7134e-01,\n",
      "        -1.7726e-01, -4.3721e-01,  3.1901e-01, -1.0492e-01,  1.3776e-01,\n",
      "        -2.1972e-01, -1.1556e-03, -2.5787e-01, -2.0966e-01, -2.3607e-02,\n",
      "         5.4112e-01, -3.0758e-01, -3.9031e-01, -4.9369e-01,  3.1366e-01,\n",
      "        -1.5224e-01, -2.4696e-01, -6.5543e-01,  1.4097e-01, -1.0754e-01,\n",
      "         1.1012e-01, -3.9968e-01,  2.7706e-01,  3.6072e-01, -1.6566e-03],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.1888,  1.8744, -1.9923,  ..., -2.3100, -0.5336,  2.2872],\n",
      "        [-4.9470, -0.1464, -2.7042,  ..., -4.8377, -1.0255,  4.3143],\n",
      "        [-1.3095,  0.3845, -2.0527,  ..., -2.0641,  0.4925, -0.0841],\n",
      "        ...,\n",
      "        [ 1.8260,  2.5640,  0.0706,  ..., -1.5746, -0.7952,  1.1487],\n",
      "        [ 0.4517,  0.7917, -1.9746,  ..., -2.5260,  0.3723,  2.0174],\n",
      "        [ 0.6205,  0.9882,  0.7785,  ..., -2.1545, -0.2392,  2.6561]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(z_images_pos, z_images_neg)\n",
    "print(tag_features[0])\n",
    "print(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e12f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f995917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87de29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfbc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62ca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0988084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3291fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
